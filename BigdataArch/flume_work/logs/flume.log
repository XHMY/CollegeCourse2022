17 Jun 2022 10:44:58,305 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
17 Jun 2022 10:44:58,315 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:conf/hive.conf
17 Jun 2022 10:44:58,319 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:k1
17 Jun 2022 10:44:58,320 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:r4
17 Jun 2022 10:44:58,320 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:r1
17 Jun 2022 10:44:58,320 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:k1
17 Jun 2022 10:44:58,321 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:k1
17 Jun 2022 10:44:58,321 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:k1
17 Jun 2022 10:44:58,321 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:k1
17 Jun 2022 10:44:58,322 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:c1
17 Jun 2022 10:44:58,322 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:k1
17 Jun 2022 10:44:58,322 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:c1
17 Jun 2022 10:44:58,323 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:k1
17 Jun 2022 10:44:58,323 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:k1
17 Jun 2022 10:44:58,323 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:r4
17 Jun 2022 10:44:58,323 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:c1
17 Jun 2022 10:44:58,323 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:r4
17 Jun 2022 10:44:58,323 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: k1 Agent: a1
17 Jun 2022 10:44:58,323 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:r4
17 Jun 2022 10:44:58,323 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:k1
17 Jun 2022 10:44:58,324 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:r4
17 Jun 2022 10:44:58,324 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:k1
17 Jun 2022 10:44:58,324 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:k1
17 Jun 2022 10:44:58,324 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:r4
17 Jun 2022 10:44:58,324 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:k1
17 Jun 2022 10:44:58,324 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:k1
17 Jun 2022 10:44:58,324 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'a1' has no configfilters.
17 Jun 2022 10:44:58,334 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [a1]
17 Jun 2022 10:44:58,334 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
17 Jun 2022 10:44:58,339 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel c1 type memory
17 Jun 2022 10:44:58,343 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel c1
17 Jun 2022 10:44:58,345 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source r4, type TAILDIR
17 Jun 2022 10:44:58,359 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: k1, type: hive
17 Jun 2022 10:44:58,368 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel c1 connected to [r4, k1]
17 Jun 2022 10:44:58,369 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{r4=PollableSourceRunner: { source:Taildir source: { positionFile: /Users/yokey/.flume/taildir_position.json, skipToEnd: false, byteOffsetHeader: false, idleTimeout: 120000, writePosInterval: 3000 } counterGroup:{ name:null counters:{} } }} sinkRunners:{k1=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@77c174d2 counterGroup:{ name:null counters:{} } }} channels:{c1=org.apache.flume.channel.MemoryChannel{name: c1}} }
17 Jun 2022 10:44:58,370 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel c1
17 Jun 2022 10:44:58,371 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: c1: Successfully registered new MBean.
17 Jun 2022 10:44:58,372 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: c1 started
17 Jun 2022 10:44:58,372 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink k1
17 Jun 2022 10:44:58,372 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source r4
17 Jun 2022 10:44:58,372 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: k1: Successfully registered new MBean.
17 Jun 2022 10:44:58,373 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.source.taildir.TaildirSource.start:94)  - r4 TaildirSource source starting with directory: {f1=/usr/local/var/log/nginx/access.log}
17 Jun 2022 10:44:58,373 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: k1 started
17 Jun 2022 10:44:58,373 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.sink.hive.HiveSink.start:502)  - k1: Hive Sink k1 started
17 Jun 2022 10:44:58,377 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.source.taildir.ReliableTaildirEventReader.<init>:84)  - taildirCache: [{filegroup='f1', filePattern='/usr/local/var/log/nginx/access.log', cached=true}]
17 Jun 2022 10:44:58,379 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.source.taildir.ReliableTaildirEventReader.<init>:85)  - headerTable: {}
17 Jun 2022 10:44:58,385 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.source.taildir.ReliableTaildirEventReader.openFile:290)  - Opening file: /usr/local/var/log/nginx/access.log, inode: 72303316, pos: 0
17 Jun 2022 10:44:58,385 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.source.taildir.ReliableTaildirEventReader.<init>:95)  - Updating position from position file: /Users/yokey/.flume/taildir_position.json
17 Jun 2022 10:44:58,393 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.source.taildir.TailFile.updatePos:126)  - Updated position, file: /usr/local/var/log/nginx/access.log, inode: 72303316, pos: 2441
17 Jun 2022 10:44:58,394 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SOURCE, name: r4: Successfully registered new MBean.
17 Jun 2022 10:44:58,394 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SOURCE, name: r4 started
17 Jun 2022 10:46:58,476 INFO  [PollableSourceRunner-TaildirSource-r4] (org.apache.flume.source.taildir.TaildirSource.closeTailFiles:307)  - Closed file: /usr/local/var/log/nginx/access.log, inode: 72303316, pos: 2441
17 Jun 2022 10:58:23,930 INFO  [PollableSourceRunner-TaildirSource-r4] (org.apache.flume.source.taildir.ReliableTaildirEventReader.openFile:290)  - Opening file: /usr/local/var/log/nginx/access.log, inode: 72303316, pos: 2441
17 Jun 2022 10:58:25,134 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hive.HiveSink.getOrCreateWriter:344)  - k1: Creating Writer to Hive end point : {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-10-50] }
17 Jun 2022 10:58:25,670 INFO  [hive-k1-call-runner-0] (org.apache.hive.hcatalog.common.HiveClientCache.<init>:119)  - Initializing cache: eviction-timeout=120 initial-capacity=50 maximum-capacity=50
17 Jun 2022 10:58:25,756 WARN  [hive-k1-call-runner-0] (org.apache.hadoop.util.NativeCodeLoader.<clinit>:60)  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17 Jun 2022 10:58:28,308 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hive.HiveWriter.nextTxnBatch:398)  - Acquired Transaction batch TxnId/WriteIds=[1029/201...1128/300] on endPoint = {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-10-50] };  TxnStatus[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] LastUsed txnid:0
17 Jun 2022 10:58:31,723 INFO  [hive-k1-call-runner-0] (org.apache.orc.impl.PhysicalFsWriter.<init>:92)  - ORC writer created for path: hdfs://0.0.0.0:9000/user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-10-50/delta_0000201_0000300/bucket_00001 with stripeSize: 8388608 blockSize: 268435456 compression: ZLIB bufferSize: 32768
17 Jun 2022 10:58:31,730 INFO  [hive-k1-call-runner-0] (org.apache.orc.impl.OrcCodecPool.getCodec:56)  - Got brand-new codec ZLIB
17 Jun 2022 10:58:31,766 INFO  [hive-k1-call-runner-0] (org.apache.orc.impl.WriterImpl.<init>:196)  - ORC writer created for path: hdfs://0.0.0.0:9000/user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-10-50/delta_0000201_0000300/bucket_00001 with stripeSize: 8388608 blockSize: 268435456 compression: ZLIB bufferSize: 32768
17 Jun 2022 10:58:31,791 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hive.HiveWriter.commitTxn:337)  - Committing Txn 1029 on EndPoint: {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-10-50] }
17 Jun 2022 11:00:29,000 INFO  [PollableSourceRunner-TaildirSource-r4] (org.apache.flume.source.taildir.TaildirSource.closeTailFiles:307)  - Closed file: /usr/local/var/log/nginx/access.log, inode: 72303316, pos: 2839
17 Jun 2022 11:20:34,703 INFO  [PollableSourceRunner-TaildirSource-r4] (org.apache.flume.source.taildir.ReliableTaildirEventReader.openFile:290)  - Opening file: /usr/local/var/log/nginx/access.log, inode: 72303316, pos: 2839
17 Jun 2022 11:20:38,747 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hive.HiveSink.getOrCreateWriter:344)  - k1: Creating Writer to Hive end point : {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-11-20] }
17 Jun 2022 11:20:39,581 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hive.HiveWriter.nextTxnBatch:398)  - Acquired Transaction batch TxnId/WriteIds=[1134/301...1233/400] on endPoint = {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-11-20] };  TxnStatus[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] LastUsed txnid:0
17 Jun 2022 11:20:42,737 INFO  [hive-k1-call-runner-0] (org.apache.orc.impl.PhysicalFsWriter.<init>:92)  - ORC writer created for path: hdfs://0.0.0.0:9000/user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-20/delta_0000301_0000400/bucket_00001 with stripeSize: 8388608 blockSize: 268435456 compression: ZLIB bufferSize: 32768
17 Jun 2022 11:20:42,739 INFO  [hive-k1-call-runner-0] (org.apache.orc.impl.OrcCodecPool.getCodec:56)  - Got brand-new codec ZLIB
17 Jun 2022 11:20:42,741 INFO  [hive-k1-call-runner-0] (org.apache.orc.impl.WriterImpl.<init>:196)  - ORC writer created for path: hdfs://0.0.0.0:9000/user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-20/delta_0000301_0000400/bucket_00001 with stripeSize: 8388608 blockSize: 268435456 compression: ZLIB bufferSize: 32768
17 Jun 2022 11:20:42,751 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hive.HiveWriter.commitTxn:337)  - Committing Txn 1134 on EndPoint: {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-11-20] }
17 Jun 2022 11:22:44,803 INFO  [PollableSourceRunner-TaildirSource-r4] (org.apache.flume.source.taildir.TaildirSource.closeTailFiles:307)  - Closed file: /usr/local/var/log/nginx/access.log, inode: 72303316, pos: 3209
17 Jun 2022 11:28:10,001 INFO  [PollableSourceRunner-TaildirSource-r4] (org.apache.flume.source.taildir.ReliableTaildirEventReader.openFile:290)  - Opening file: /usr/local/var/log/nginx/access.log, inode: 72303316, pos: 3209
17 Jun 2022 11:28:13,021 INFO  [hive-k1-call-runner-0] (org.apache.orc.impl.PhysicalFsWriter.<init>:92)  - ORC writer created for path: hdfs://0.0.0.0:9000/user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-20/delta_0000301_0000400/bucket_00002 with stripeSize: 8388608 blockSize: 268435456 compression: ZLIB bufferSize: 32768
17 Jun 2022 11:28:13,023 INFO  [hive-k1-call-runner-0] (org.apache.orc.impl.OrcCodecPool.getCodec:56)  - Got brand-new codec ZLIB
17 Jun 2022 11:28:13,025 INFO  [hive-k1-call-runner-0] (org.apache.orc.impl.WriterImpl.<init>:196)  - ORC writer created for path: hdfs://0.0.0.0:9000/user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-20/delta_0000301_0000400/bucket_00002 with stripeSize: 8388608 blockSize: 268435456 compression: ZLIB bufferSize: 32768
17 Jun 2022 11:28:13,028 INFO  [hive-k1-call-runner-0] (org.apache.flume.sink.hive.HiveWriter$2.call:236)  - Sending heartbeat on batch TxnId/WriteIds=[1134/301...1233/400] on endPoint = {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-11-20] };  TxnStatus[COOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] LastUsed txnid:1135
17 Jun 2022 11:28:13,081 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hive.HiveWriter.commitTxn:337)  - Committing Txn 1135 on EndPoint: {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-11-20] }
17 Jun 2022 11:29:58,213 INFO  [hive-k1-call-runner-0] (org.apache.flume.sink.hive.HiveWriter$2.call:236)  - Sending heartbeat on batch TxnId/WriteIds=[1134/301...1233/400] on endPoint = {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-11-20] };  TxnStatus[CCOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] LastUsed txnid:1136
17 Jun 2022 11:29:58,220 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hive.HiveWriter.commitTxn:337)  - Committing Txn 1136 on EndPoint: {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-11-20] }
17 Jun 2022 11:30:52,307 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hive.HiveSink.getOrCreateWriter:344)  - k1: Creating Writer to Hive end point : {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-11-30] }
17 Jun 2022 11:30:52,616 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hive.HiveWriter.nextTxnBatch:398)  - Acquired Transaction batch TxnId/WriteIds=[1238/401...1337/500] on endPoint = {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-11-30] };  TxnStatus[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] LastUsed txnid:0
17 Jun 2022 11:30:55,652 INFO  [hive-k1-call-runner-0] (org.apache.orc.impl.PhysicalFsWriter.<init>:92)  - ORC writer created for path: hdfs://0.0.0.0:9000/user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-30/delta_0000401_0000500/bucket_00000 with stripeSize: 8388608 blockSize: 268435456 compression: ZLIB bufferSize: 32768
17 Jun 2022 11:30:55,659 INFO  [hive-k1-call-runner-0] (org.apache.orc.impl.OrcCodecPool.getCodec:56)  - Got brand-new codec ZLIB
17 Jun 2022 11:30:55,663 INFO  [hive-k1-call-runner-0] (org.apache.orc.impl.WriterImpl.<init>:196)  - ORC writer created for path: hdfs://0.0.0.0:9000/user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-30/delta_0000401_0000500/bucket_00000 with stripeSize: 8388608 blockSize: 268435456 compression: ZLIB bufferSize: 32768
17 Jun 2022 11:30:55,693 INFO  [hive-k1-call-runner-0] (org.apache.orc.impl.PhysicalFsWriter.<init>:92)  - ORC writer created for path: hdfs://0.0.0.0:9000/user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-30/delta_0000401_0000500/bucket_00002 with stripeSize: 8388608 blockSize: 268435456 compression: ZLIB bufferSize: 32768
17 Jun 2022 11:30:55,695 INFO  [hive-k1-call-runner-0] (org.apache.orc.impl.OrcCodecPool.getCodec:56)  - Got brand-new codec ZLIB
17 Jun 2022 11:30:55,696 INFO  [hive-k1-call-runner-0] (org.apache.orc.impl.WriterImpl.<init>:196)  - ORC writer created for path: hdfs://0.0.0.0:9000/user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-30/delta_0000401_0000500/bucket_00002 with stripeSize: 8388608 blockSize: 268435456 compression: ZLIB bufferSize: 32768
17 Jun 2022 11:30:55,708 INFO  [hive-k1-call-runner-0] (org.apache.orc.impl.PhysicalFsWriter.<init>:92)  - ORC writer created for path: hdfs://0.0.0.0:9000/user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-30/delta_0000401_0000500/bucket_00004 with stripeSize: 8388608 blockSize: 268435456 compression: ZLIB bufferSize: 32768
17 Jun 2022 11:30:55,712 INFO  [hive-k1-call-runner-0] (org.apache.orc.impl.OrcCodecPool.getCodec:56)  - Got brand-new codec ZLIB
17 Jun 2022 11:30:55,714 INFO  [hive-k1-call-runner-0] (org.apache.orc.impl.WriterImpl.<init>:196)  - ORC writer created for path: hdfs://0.0.0.0:9000/user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-30/delta_0000401_0000500/bucket_00004 with stripeSize: 8388608 blockSize: 268435456 compression: ZLIB bufferSize: 32768
17 Jun 2022 11:30:55,730 INFO  [hive-k1-call-runner-0] (org.apache.orc.impl.PhysicalFsWriter.<init>:92)  - ORC writer created for path: hdfs://0.0.0.0:9000/user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-30/delta_0000401_0000500/bucket_00003 with stripeSize: 8388608 blockSize: 268435456 compression: ZLIB bufferSize: 32768
17 Jun 2022 11:30:55,732 INFO  [hive-k1-call-runner-0] (org.apache.orc.impl.OrcCodecPool.getCodec:56)  - Got brand-new codec ZLIB
17 Jun 2022 11:30:55,733 INFO  [hive-k1-call-runner-0] (org.apache.orc.impl.WriterImpl.<init>:196)  - ORC writer created for path: hdfs://0.0.0.0:9000/user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-30/delta_0000401_0000500/bucket_00003 with stripeSize: 8388608 blockSize: 268435456 compression: ZLIB bufferSize: 32768
17 Jun 2022 11:30:55,734 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hive.HiveWriter.commitTxn:337)  - Committing Txn 1238 on EndPoint: {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-11-30] }
17 Jun 2022 11:32:55,155 INFO  [PollableSourceRunner-TaildirSource-r4] (org.apache.flume.source.taildir.TaildirSource.closeTailFiles:307)  - Closed file: /usr/local/var/log/nginx/access.log, inode: 72303316, pos: 9328
17 Jun 2022 13:35:58,165 WARN  [ResponseProcessor for block BP-1769164461-192.168.1.102-1655303670456:blk_1073742156_1332] (org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run:1245)  - Exception for BP-1769164461-192.168.1.102-1655303670456:blk_1073742156_1332
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:521)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
	at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1137)
17 Jun 2022 13:35:58,165 WARN  [ResponseProcessor for block BP-1769164461-192.168.1.102-1655303670456:blk_1073741966_1142] (org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run:1245)  - Exception for BP-1769164461-192.168.1.102-1655303670456:blk_1073741966_1142
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:521)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
	at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1137)
17 Jun 2022 13:35:58,165 WARN  [ResponseProcessor for block BP-1769164461-192.168.1.102-1655303670456:blk_1073742217_1393] (org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run:1245)  - Exception for BP-1769164461-192.168.1.102-1655303670456:blk_1073742217_1393
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:521)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
	at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1137)
17 Jun 2022 13:35:58,165 WARN  [ResponseProcessor for block BP-1769164461-192.168.1.102-1655303670456:blk_1073742214_1390] (org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run:1245)  - Exception for BP-1769164461-192.168.1.102-1655303670456:blk_1073742214_1390
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:521)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
	at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1137)
17 Jun 2022 13:35:58,165 WARN  [ResponseProcessor for block BP-1769164461-192.168.1.102-1655303670456:blk_1073742210_1386] (org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run:1245)  - Exception for BP-1769164461-192.168.1.102-1655303670456:blk_1073742210_1386
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:521)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
	at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1137)
17 Jun 2022 13:35:58,165 WARN  [ResponseProcessor for block BP-1769164461-192.168.1.102-1655303670456:blk_1073742154_1330] (org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run:1245)  - Exception for BP-1769164461-192.168.1.102-1655303670456:blk_1073742154_1330
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:521)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
	at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1137)
17 Jun 2022 13:35:58,165 WARN  [ResponseProcessor for block BP-1769164461-192.168.1.102-1655303670456:blk_1073742212_1388] (org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run:1245)  - Exception for BP-1769164461-192.168.1.102-1655303670456:blk_1073742212_1388
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:521)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
	at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1137)
17 Jun 2022 13:35:58,165 WARN  [ResponseProcessor for block BP-1769164461-192.168.1.102-1655303670456:blk_1073741968_1144] (org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run:1245)  - Exception for BP-1769164461-192.168.1.102-1655303670456:blk_1073741968_1144
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:521)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
	at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1137)
17 Jun 2022 13:35:58,165 WARN  [ResponseProcessor for block BP-1769164461-192.168.1.102-1655303670456:blk_1073742216_1392] (org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run:1245)  - Exception for BP-1769164461-192.168.1.102-1655303670456:blk_1073742216_1392
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:521)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
	at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1137)
17 Jun 2022 13:35:58,165 WARN  [ResponseProcessor for block BP-1769164461-192.168.1.102-1655303670456:blk_1073742193_1369] (org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run:1245)  - Exception for BP-1769164461-192.168.1.102-1655303670456:blk_1073742193_1369
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:521)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
	at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1137)
17 Jun 2022 13:35:58,165 WARN  [ResponseProcessor for block BP-1769164461-192.168.1.102-1655303670456:blk_1073742218_1394] (org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run:1245)  - Exception for BP-1769164461-192.168.1.102-1655303670456:blk_1073742218_1394
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:521)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
	at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1137)
17 Jun 2022 13:35:58,165 WARN  [ResponseProcessor for block BP-1769164461-192.168.1.102-1655303670456:blk_1073742213_1389] (org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run:1245)  - Exception for BP-1769164461-192.168.1.102-1655303670456:blk_1073742213_1389
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:521)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
	at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1137)
17 Jun 2022 13:35:58,165 WARN  [ResponseProcessor for block BP-1769164461-192.168.1.102-1655303670456:blk_1073742215_1391] (org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run:1245)  - Exception for BP-1769164461-192.168.1.102-1655303670456:blk_1073742215_1391
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:521)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
	at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1137)
17 Jun 2022 13:35:58,165 WARN  [ResponseProcessor for block BP-1769164461-192.168.1.102-1655303670456:blk_1073742194_1370] (org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run:1245)  - Exception for BP-1769164461-192.168.1.102-1655303670456:blk_1073742194_1370
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:521)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
	at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1137)
17 Jun 2022 13:36:00,353 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{r4=PollableSourceRunner: { source:Taildir source: { positionFile: /Users/yokey/.flume/taildir_position.json, skipToEnd: false, byteOffsetHeader: false, idleTimeout: 120000, writePosInterval: 3000 } counterGroup:{ name:null counters:{runner.backoffs.consecutive=1060, runner.polls=1060, runner.backoffs=1060} } }} sinkRunners:{k1=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@77c174d2 counterGroup:{ name:null counters:{runner.backoffs.consecutive=318, runner.backoffs=666} } }} channels:{c1=org.apache.flume.channel.MemoryChannel{name: c1}} }
17 Jun 2022 13:36:00,354 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source r4
17 Jun 2022 13:36:00,355 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: PollableSourceRunner: { source:Taildir source: { positionFile: /Users/yokey/.flume/taildir_position.json, skipToEnd: false, byteOffsetHeader: false, idleTimeout: 120000, writePosInterval: 3000 } counterGroup:{ name:null counters:{runner.backoffs.consecutive=1060, runner.polls=1060, runner.backoffs=1060} } }
17 Jun 2022 13:36:00,355 INFO  [PollableSourceRunner-TaildirSource-r4] (org.apache.flume.source.PollableSourceRunner$PollingRunner.run:143)  - Source runner interrupted. Exiting
17 Jun 2022 13:36:00,357 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SOURCE, name: r4 stopped
17 Jun 2022 13:36:00,358 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SOURCE, name: r4. source.start.time == 1655433898394
17 Jun 2022 13:36:00,358 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SOURCE, name: r4. source.stop.time == 1655444160357
17 Jun 2022 13:36:00,358 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SOURCE, name: r4. src.append-batch.accepted == 6
17 Jun 2022 13:36:00,359 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SOURCE, name: r4. src.append-batch.received == 6
17 Jun 2022 13:36:00,359 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SOURCE, name: r4. src.append.accepted == 0
17 Jun 2022 13:36:00,359 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SOURCE, name: r4. src.append.received == 0
17 Jun 2022 13:36:00,359 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SOURCE, name: r4. src.channel.write.fail == 0
17 Jun 2022 13:36:00,359 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SOURCE, name: r4. src.event.read.fail == 0
17 Jun 2022 13:36:00,359 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SOURCE, name: r4. src.events.accepted == 34
17 Jun 2022 13:36:00,359 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SOURCE, name: r4. src.events.received == 34
17 Jun 2022 13:36:00,359 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SOURCE, name: r4. src.generic.processing.fail == 0
17 Jun 2022 13:36:00,360 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SOURCE, name: r4. src.open-connection.count == 0
17 Jun 2022 13:36:00,360 INFO  [agent-shutdown-hook] (org.apache.flume.source.taildir.TaildirSource.stop:144)  - Taildir source r4 stopped. Metrics: SOURCE:r4{src.events.accepted=34, src.open-connection.count=0, src.append.received=0, src.channel.write.fail=0, src.append-batch.received=6, src.generic.processing.fail=0, src.append-batch.accepted=6, src.append.accepted=0, src.events.received=34, src.event.read.fail=0}
17 Jun 2022 13:36:00,360 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink k1
17 Jun 2022 13:36:00,360 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@77c174d2 counterGroup:{ name:null counters:{runner.backoffs.consecutive=318, runner.backoffs=666} } }
17 Jun 2022 13:36:00,370 ERROR [shutdown-hook-0] (org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten:644)  - Failed to close file: /user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-20/delta_0000301_0000400/bucket_00001 with inode: 17440
java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:9866,DS-e4cbb7d1-f7c3-4d33-927d-3c501282ccde,DISK]] are bad. Aborting...
	at org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1609)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1543)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1529)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1305)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:668)
17 Jun 2022 13:36:00,371 ERROR [shutdown-hook-0] (org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten:644)  - Failed to close file: /user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-30/delta_0000401_0000500/bucket_00002 with inode: 17568
java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:9866,DS-e4cbb7d1-f7c3-4d33-927d-3c501282ccde,DISK]] are bad. Aborting...
	at org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1609)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1543)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1529)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1305)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:668)
17 Jun 2022 13:36:00,371 ERROR [shutdown-hook-0] (org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten:644)  - Failed to close file: /user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-10-50/delta_0000201_0000300/bucket_00001_flush_length with inode: 16993
java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:9866,DS-e4cbb7d1-f7c3-4d33-927d-3c501282ccde,DISK]] are bad. Aborting...
	at org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1609)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1543)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1529)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1305)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:668)
17 Jun 2022 13:36:00,371 ERROR [shutdown-hook-0] (org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten:644)  - Failed to close file: /user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-30/delta_0000401_0000500/bucket_00004_flush_length with inode: 17569
java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:9866,DS-e4cbb7d1-f7c3-4d33-927d-3c501282ccde,DISK]] are bad. Aborting...
	at org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1609)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1543)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1529)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1305)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:668)
17 Jun 2022 13:36:00,371 ERROR [shutdown-hook-0] (org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten:644)  - Failed to close file: /user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-10-50/delta_0000201_0000300/bucket_00001 with inode: 16994
java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:9866,DS-e4cbb7d1-f7c3-4d33-927d-3c501282ccde,DISK]] are bad. Aborting...
	at org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1609)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1543)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1529)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1305)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:668)
17 Jun 2022 13:36:00,372 ERROR [shutdown-hook-0] (org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten:644)  - Failed to close file: /user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-30/delta_0000401_0000500/bucket_00004 with inode: 17570
java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:9866,DS-e4cbb7d1-f7c3-4d33-927d-3c501282ccde,DISK]] are bad. Aborting...
	at org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1609)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1543)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1529)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1305)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:668)
17 Jun 2022 13:36:00,372 ERROR [shutdown-hook-0] (org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten:644)  - Failed to close file: /user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-30/delta_0000401_0000500/bucket_00003_flush_length with inode: 17571
java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:9866,DS-e4cbb7d1-f7c3-4d33-927d-3c501282ccde,DISK]] are bad. Aborting...
	at org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1609)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1543)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1529)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1305)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:668)
17 Jun 2022 13:36:00,372 ERROR [shutdown-hook-0] (org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten:644)  - Failed to close file: /user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-30/delta_0000401_0000500/bucket_00003 with inode: 17572
java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:9866,DS-e4cbb7d1-f7c3-4d33-927d-3c501282ccde,DISK]] are bad. Aborting...
	at org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1609)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1543)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1529)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1305)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:668)
17 Jun 2022 13:36:00,372 ERROR [shutdown-hook-0] (org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten:644)  - Failed to close file: /user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-20/delta_0000301_0000400/bucket_00002_flush_length with inode: 17518
java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:9866,DS-e4cbb7d1-f7c3-4d33-927d-3c501282ccde,DISK]] are bad. Aborting...
	at org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1609)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1543)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1529)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1305)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:668)
17 Jun 2022 13:36:00,372 ERROR [shutdown-hook-0] (org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten:644)  - Failed to close file: /user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-20/delta_0000301_0000400/bucket_00002 with inode: 17519
java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:9866,DS-e4cbb7d1-f7c3-4d33-927d-3c501282ccde,DISK]] are bad. Aborting...
	at org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1609)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1543)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1529)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1305)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:668)
17 Jun 2022 13:36:00,373 ERROR [shutdown-hook-0] (org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten:644)  - Failed to close file: /user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-30/delta_0000401_0000500/bucket_00000_flush_length with inode: 17563
java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:9866,DS-e4cbb7d1-f7c3-4d33-927d-3c501282ccde,DISK]] are bad. Aborting...
	at org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1609)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1543)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1529)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1305)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:668)
17 Jun 2022 13:36:00,373 ERROR [shutdown-hook-0] (org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten:644)  - Failed to close file: /user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-30/delta_0000401_0000500/bucket_00000 with inode: 17564
java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:9866,DS-e4cbb7d1-f7c3-4d33-927d-3c501282ccde,DISK]] are bad. Aborting...
	at org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1609)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1543)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1529)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1305)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:668)
17 Jun 2022 13:36:00,373 ERROR [shutdown-hook-0] (org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten:644)  - Failed to close file: /user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-30/delta_0000401_0000500/bucket_00002_flush_length with inode: 17566
java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:9866,DS-e4cbb7d1-f7c3-4d33-927d-3c501282ccde,DISK]] are bad. Aborting...
	at org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1609)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1543)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1529)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1305)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:668)
17 Jun 2022 13:36:00,373 ERROR [shutdown-hook-0] (org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten:644)  - Failed to close file: /user/hive/warehouse/logsdb.db/weblogs/log_time=22-06-17-11-20/delta_0000301_0000400/bucket_00001_flush_length with inode: 17439
java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:9866,DS-e4cbb7d1-f7c3-4d33-927d-3c501282ccde,DISK]] are bad. Aborting...
	at org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1609)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1543)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1529)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1305)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:668)
17 Jun 2022 13:36:04,398 WARN  [agent-shutdown-hook] (org.apache.flume.sink.hive.HiveWriter.abortCurrTxnHelper:303)  - Unable to abort transaction 1137
org.apache.hive.hcatalog.streaming.TransactionError: Unable to abort transaction id : 1137: Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:226)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:516)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:379)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient$1.run(RetryingMetaStoreClient.java:187)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:565)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:183)
	at jdk.proxy2/jdk.proxy2.$Proxy9.rollbackTxn(Unknown Source)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:972)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abort(HiveEndPoint.java:936)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abort(HiveEndPoint.java:931)
	at org.apache.flume.sink.hive.HiveWriter$4.call(HiveWriter.java:296)
	at org.apache.flume.sink.hive.HiveWriter$4.call(HiveWriter.java:293)
	at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:428)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:546)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:594)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:221)
	... 18 more

	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:983)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abort(HiveEndPoint.java:936)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abort(HiveEndPoint.java:931)
	at org.apache.flume.sink.hive.HiveWriter$4.call(HiveWriter.java:296)
	at org.apache.flume.sink.hive.HiveWriter$4.call(HiveWriter.java:293)
	at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:428)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:226)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:516)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:379)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient$1.run(RetryingMetaStoreClient.java:187)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:565)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:183)
	at jdk.proxy2/jdk.proxy2.$Proxy9.rollbackTxn(Unknown Source)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:972)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abort(HiveEndPoint.java:936)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abort(HiveEndPoint.java:931)
	at org.apache.flume.sink.hive.HiveWriter$4.call(HiveWriter.java:296)
	at org.apache.flume.sink.hive.HiveWriter$4.call(HiveWriter.java:293)
	at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:428)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:546)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:594)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:221)
	... 18 more
)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:565)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:379)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient$1.run(RetryingMetaStoreClient.java:187)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:565)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:183)
	at jdk.proxy2/jdk.proxy2.$Proxy9.rollbackTxn(Unknown Source)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:972)
	... 9 more
17 Jun 2022 13:36:04,401 WARN  [agent-shutdown-hook] (org.apache.flume.sink.hive.HiveWriter.abortRemainingTxns:282)  - Error when aborting remaining transactions in batch TxnId/WriteIds=[1134/301...1233/400] on endPoint = {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-11-20] };  TxnStatus[CCCOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] LastUsed txnid:1138
org.apache.hive.hcatalog.streaming.TransactionError: Unable to acquire lock on {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-11-20] }: Cannot write to null outputStream
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.beginNextTransactionImpl(HiveEndPoint.java:714)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.beginNextTransaction(HiveEndPoint.java:678)
	at org.apache.flume.sink.hive.HiveWriter$3.call(HiveWriter.java:275)
	at org.apache.flume.sink.hive.HiveWriter$3.call(HiveWriter.java:272)
	at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:428)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.thrift.transport.TTransportException: Cannot write to null outputStream
	at org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:142)
	at org.apache.thrift.protocol.TBinaryProtocol.writeI32(TBinaryProtocol.java:178)
	at org.apache.thrift.protocol.TBinaryProtocol.writeMessageBegin(TBinaryProtocol.java:106)
	at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:70)
	at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:62)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.send_lock(ThriftHiveMetastore.java:5293)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.lock(ThriftHiveMetastore.java:5285)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.lock(HiveMetaStoreClient.java:2568)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)
	at java.base/java.lang.reflect.Method.invoke(Method.java:577)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:208)
	at jdk.proxy2/jdk.proxy2.$Proxy9.lock(Unknown Source)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.beginNextTransactionImpl(HiveEndPoint.java:709)
	... 8 more
17 Jun 2022 13:36:04,401 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hive.HiveWriter.closeTxnBatch:407)  - Closing Txn Batch TxnId/WriteIds=[1134/301...1233/400] on endPoint = {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-11-20] };  TxnStatus[CCCOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] LastUsed txnid:1138.
17 Jun 2022 13:36:08,418 WARN  [agent-shutdown-hook] (org.apache.flume.sink.hive.HiveWriter.closeTxnBatch:418)  - Error closing Txn Batch TxnId/WriteIds=[1134/301...1233/400] on endPoint = {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-11-20] };  TxnStatus[CCCOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] LastUsed txnid:1138
org.apache.hive.hcatalog.streaming.TransactionError: Unable to abort transaction id : 1138: Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:226)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:516)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:379)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient$1.run(RetryingMetaStoreClient.java:187)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:565)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:183)
	at jdk.proxy2/jdk.proxy2.$Proxy9.rollbackTxn(Unknown Source)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:965)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.close(HiveEndPoint.java:1026)
	at org.apache.flume.sink.hive.HiveWriter$10.call(HiveWriter.java:411)
	at org.apache.flume.sink.hive.HiveWriter$10.call(HiveWriter.java:408)
	at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:428)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:546)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:594)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:221)
	... 17 more

	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:983)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.close(HiveEndPoint.java:1026)
	at org.apache.flume.sink.hive.HiveWriter$10.call(HiveWriter.java:411)
	at org.apache.flume.sink.hive.HiveWriter$10.call(HiveWriter.java:408)
	at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:428)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:226)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:516)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:379)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient$1.run(RetryingMetaStoreClient.java:187)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:565)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:183)
	at jdk.proxy2/jdk.proxy2.$Proxy9.rollbackTxn(Unknown Source)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:965)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.close(HiveEndPoint.java:1026)
	at org.apache.flume.sink.hive.HiveWriter$10.call(HiveWriter.java:411)
	at org.apache.flume.sink.hive.HiveWriter$10.call(HiveWriter.java:408)
	at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:428)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:546)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:594)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:221)
	... 17 more
)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:565)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:379)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient$1.run(RetryingMetaStoreClient.java:187)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:565)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:183)
	at jdk.proxy2/jdk.proxy2.$Proxy9.rollbackTxn(Unknown Source)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:965)
	... 8 more
17 Jun 2022 13:36:08,419 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hive.HiveWriter.closeConnection:319)  - Closing connection to EndPoint : {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-11-20] }
17 Jun 2022 13:36:12,437 WARN  [agent-shutdown-hook] (org.apache.flume.sink.hive.HiveWriter.abortCurrTxnHelper:303)  - Unable to abort transaction 1239
org.apache.hive.hcatalog.streaming.TransactionError: Unable to abort transaction id : 1239: Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:226)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:516)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:379)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient$1.run(RetryingMetaStoreClient.java:187)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:565)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:183)
	at jdk.proxy2/jdk.proxy2.$Proxy9.rollbackTxn(Unknown Source)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:972)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abort(HiveEndPoint.java:936)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abort(HiveEndPoint.java:931)
	at org.apache.flume.sink.hive.HiveWriter$4.call(HiveWriter.java:296)
	at org.apache.flume.sink.hive.HiveWriter$4.call(HiveWriter.java:293)
	at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:428)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:546)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:594)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:221)
	... 18 more

	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:983)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abort(HiveEndPoint.java:936)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abort(HiveEndPoint.java:931)
	at org.apache.flume.sink.hive.HiveWriter$4.call(HiveWriter.java:296)
	at org.apache.flume.sink.hive.HiveWriter$4.call(HiveWriter.java:293)
	at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:428)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:226)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:516)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:379)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient$1.run(RetryingMetaStoreClient.java:187)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:565)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:183)
	at jdk.proxy2/jdk.proxy2.$Proxy9.rollbackTxn(Unknown Source)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:972)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abort(HiveEndPoint.java:936)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abort(HiveEndPoint.java:931)
	at org.apache.flume.sink.hive.HiveWriter$4.call(HiveWriter.java:296)
	at org.apache.flume.sink.hive.HiveWriter$4.call(HiveWriter.java:293)
	at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:428)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:546)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:594)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:221)
	... 18 more
)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:565)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:379)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient$1.run(RetryingMetaStoreClient.java:187)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:565)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:183)
	at jdk.proxy2/jdk.proxy2.$Proxy9.rollbackTxn(Unknown Source)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:972)
	... 9 more
17 Jun 2022 13:36:12,439 WARN  [agent-shutdown-hook] (org.apache.flume.sink.hive.HiveWriter.abortRemainingTxns:282)  - Error when aborting remaining transactions in batch TxnId/WriteIds=[1238/401...1337/500] on endPoint = {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-11-30] };  TxnStatus[COOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] LastUsed txnid:1240
org.apache.hive.hcatalog.streaming.TransactionError: Unable to acquire lock on {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-11-30] }: Cannot write to null outputStream
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.beginNextTransactionImpl(HiveEndPoint.java:714)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.beginNextTransaction(HiveEndPoint.java:678)
	at org.apache.flume.sink.hive.HiveWriter$3.call(HiveWriter.java:275)
	at org.apache.flume.sink.hive.HiveWriter$3.call(HiveWriter.java:272)
	at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:428)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.thrift.transport.TTransportException: Cannot write to null outputStream
	at org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:142)
	at org.apache.thrift.protocol.TBinaryProtocol.writeI32(TBinaryProtocol.java:178)
	at org.apache.thrift.protocol.TBinaryProtocol.writeMessageBegin(TBinaryProtocol.java:106)
	at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:70)
	at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:62)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.send_lock(ThriftHiveMetastore.java:5293)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.lock(ThriftHiveMetastore.java:5285)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.lock(HiveMetaStoreClient.java:2568)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)
	at java.base/java.lang.reflect.Method.invoke(Method.java:577)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:208)
	at jdk.proxy2/jdk.proxy2.$Proxy9.lock(Unknown Source)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.beginNextTransactionImpl(HiveEndPoint.java:709)
	... 8 more
17 Jun 2022 13:36:12,439 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hive.HiveWriter.closeTxnBatch:407)  - Closing Txn Batch TxnId/WriteIds=[1238/401...1337/500] on endPoint = {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-11-30] };  TxnStatus[COOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] LastUsed txnid:1240.
17 Jun 2022 13:36:16,458 WARN  [agent-shutdown-hook] (org.apache.flume.sink.hive.HiveWriter.closeTxnBatch:418)  - Error closing Txn Batch TxnId/WriteIds=[1238/401...1337/500] on endPoint = {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-11-30] };  TxnStatus[COOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] LastUsed txnid:1240
org.apache.hive.hcatalog.streaming.TransactionError: Unable to abort transaction id : 1240: Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:226)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:516)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:379)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient$1.run(RetryingMetaStoreClient.java:187)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:565)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:183)
	at jdk.proxy2/jdk.proxy2.$Proxy9.rollbackTxn(Unknown Source)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:965)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.close(HiveEndPoint.java:1026)
	at org.apache.flume.sink.hive.HiveWriter$10.call(HiveWriter.java:411)
	at org.apache.flume.sink.hive.HiveWriter$10.call(HiveWriter.java:408)
	at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:428)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:546)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:594)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:221)
	... 17 more

	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:983)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.close(HiveEndPoint.java:1026)
	at org.apache.flume.sink.hive.HiveWriter$10.call(HiveWriter.java:411)
	at org.apache.flume.sink.hive.HiveWriter$10.call(HiveWriter.java:408)
	at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:428)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:226)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:516)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:379)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient$1.run(RetryingMetaStoreClient.java:187)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:565)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:183)
	at jdk.proxy2/jdk.proxy2.$Proxy9.rollbackTxn(Unknown Source)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:965)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.close(HiveEndPoint.java:1026)
	at org.apache.flume.sink.hive.HiveWriter$10.call(HiveWriter.java:411)
	at org.apache.flume.sink.hive.HiveWriter$10.call(HiveWriter.java:408)
	at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:428)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:546)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:594)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:221)
	... 17 more
)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:565)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:379)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient$1.run(RetryingMetaStoreClient.java:187)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:565)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:183)
	at jdk.proxy2/jdk.proxy2.$Proxy9.rollbackTxn(Unknown Source)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:965)
	... 8 more
17 Jun 2022 13:36:16,458 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hive.HiveWriter.closeConnection:319)  - Closing connection to EndPoint : {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-11-30] }
17 Jun 2022 13:36:20,469 WARN  [agent-shutdown-hook] (org.apache.flume.sink.hive.HiveWriter.abortCurrTxnHelper:303)  - Unable to abort transaction 1030
org.apache.hive.hcatalog.streaming.TransactionError: Unable to abort transaction id : 1030: Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:226)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:516)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:379)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient$1.run(RetryingMetaStoreClient.java:187)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:565)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:183)
	at jdk.proxy2/jdk.proxy2.$Proxy9.rollbackTxn(Unknown Source)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:972)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abort(HiveEndPoint.java:936)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abort(HiveEndPoint.java:931)
	at org.apache.flume.sink.hive.HiveWriter$4.call(HiveWriter.java:296)
	at org.apache.flume.sink.hive.HiveWriter$4.call(HiveWriter.java:293)
	at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:428)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:546)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:594)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:221)
	... 18 more

	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:983)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abort(HiveEndPoint.java:936)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abort(HiveEndPoint.java:931)
	at org.apache.flume.sink.hive.HiveWriter$4.call(HiveWriter.java:296)
	at org.apache.flume.sink.hive.HiveWriter$4.call(HiveWriter.java:293)
	at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:428)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:226)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:516)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:379)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient$1.run(RetryingMetaStoreClient.java:187)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:565)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:183)
	at jdk.proxy2/jdk.proxy2.$Proxy9.rollbackTxn(Unknown Source)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:972)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abort(HiveEndPoint.java:936)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abort(HiveEndPoint.java:931)
	at org.apache.flume.sink.hive.HiveWriter$4.call(HiveWriter.java:296)
	at org.apache.flume.sink.hive.HiveWriter$4.call(HiveWriter.java:293)
	at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:428)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:546)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:594)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:221)
	... 18 more
)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:565)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:379)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient$1.run(RetryingMetaStoreClient.java:187)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:565)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:183)
	at jdk.proxy2/jdk.proxy2.$Proxy9.rollbackTxn(Unknown Source)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:972)
	... 9 more
17 Jun 2022 13:36:20,471 WARN  [agent-shutdown-hook] (org.apache.flume.sink.hive.HiveWriter.abortRemainingTxns:282)  - Error when aborting remaining transactions in batch TxnId/WriteIds=[1029/201...1128/300] on endPoint = {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-10-50] };  TxnStatus[COOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] LastUsed txnid:1031
org.apache.hive.hcatalog.streaming.TransactionError: Unable to acquire lock on {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-10-50] }: Cannot write to null outputStream
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.beginNextTransactionImpl(HiveEndPoint.java:714)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.beginNextTransaction(HiveEndPoint.java:678)
	at org.apache.flume.sink.hive.HiveWriter$3.call(HiveWriter.java:275)
	at org.apache.flume.sink.hive.HiveWriter$3.call(HiveWriter.java:272)
	at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:428)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.thrift.transport.TTransportException: Cannot write to null outputStream
	at org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:142)
	at org.apache.thrift.protocol.TBinaryProtocol.writeI32(TBinaryProtocol.java:178)
	at org.apache.thrift.protocol.TBinaryProtocol.writeMessageBegin(TBinaryProtocol.java:106)
	at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:70)
	at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:62)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.send_lock(ThriftHiveMetastore.java:5293)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.lock(ThriftHiveMetastore.java:5285)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.lock(HiveMetaStoreClient.java:2568)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)
	at java.base/java.lang.reflect.Method.invoke(Method.java:577)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:208)
	at jdk.proxy2/jdk.proxy2.$Proxy9.lock(Unknown Source)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.beginNextTransactionImpl(HiveEndPoint.java:709)
	... 8 more
17 Jun 2022 13:36:20,471 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hive.HiveWriter.closeTxnBatch:407)  - Closing Txn Batch TxnId/WriteIds=[1029/201...1128/300] on endPoint = {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-10-50] };  TxnStatus[COOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] LastUsed txnid:1031.
17 Jun 2022 13:36:24,486 WARN  [agent-shutdown-hook] (org.apache.flume.sink.hive.HiveWriter.closeTxnBatch:418)  - Error closing Txn Batch TxnId/WriteIds=[1029/201...1128/300] on endPoint = {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-10-50] };  TxnStatus[COOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] LastUsed txnid:1031
org.apache.hive.hcatalog.streaming.TransactionError: Unable to abort transaction id : 1031: Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:226)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:516)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:379)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient$1.run(RetryingMetaStoreClient.java:187)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:565)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:183)
	at jdk.proxy2/jdk.proxy2.$Proxy9.rollbackTxn(Unknown Source)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:965)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.close(HiveEndPoint.java:1026)
	at org.apache.flume.sink.hive.HiveWriter$10.call(HiveWriter.java:411)
	at org.apache.flume.sink.hive.HiveWriter$10.call(HiveWriter.java:408)
	at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:428)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:546)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:594)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:221)
	... 17 more

	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:983)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.close(HiveEndPoint.java:1026)
	at org.apache.flume.sink.hive.HiveWriter$10.call(HiveWriter.java:411)
	at org.apache.flume.sink.hive.HiveWriter$10.call(HiveWriter.java:408)
	at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:428)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:226)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:516)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:379)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient$1.run(RetryingMetaStoreClient.java:187)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:565)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:183)
	at jdk.proxy2/jdk.proxy2.$Proxy9.rollbackTxn(Unknown Source)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:965)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.close(HiveEndPoint.java:1026)
	at org.apache.flume.sink.hive.HiveWriter$10.call(HiveWriter.java:411)
	at org.apache.flume.sink.hive.HiveWriter$10.call(HiveWriter.java:408)
	at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:428)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:546)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:594)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:221)
	... 17 more
)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:565)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:379)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient$1.run(RetryingMetaStoreClient.java:187)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:565)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:183)
	at jdk.proxy2/jdk.proxy2.$Proxy9.rollbackTxn(Unknown Source)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.abortImpl(HiveEndPoint.java:965)
	... 8 more
17 Jun 2022 13:36:24,487 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hive.HiveWriter.closeConnection:319)  - Closing connection to EndPoint : {metaStoreUri='thrift://127.0.0.1:9083', database='logsdb', table='weblogs', partitionVals=[22-06-17-10-50] }
17 Jun 2022 13:36:24,487 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: k1 stopped
17 Jun 2022 13:36:24,487 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: k1. sink.start.time == 1655433898373
17 Jun 2022 13:36:24,487 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: k1. sink.stop.time == 1655444184487
17 Jun 2022 13:36:24,487 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: k1. sink.batch.complete == 0
17 Jun 2022 13:36:24,487 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: k1. sink.batch.empty == 666
17 Jun 2022 13:36:24,487 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: k1. sink.batch.underflow == 5
17 Jun 2022 13:36:24,488 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: k1. sink.channel.read.fail == 0
17 Jun 2022 13:36:24,488 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: k1. sink.connection.closed.count == 3
17 Jun 2022 13:36:24,488 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: k1. sink.connection.creation.count == 3
17 Jun 2022 13:36:24,488 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: k1. sink.connection.failed.count == 9
17 Jun 2022 13:36:24,488 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: k1. sink.event.drain.attempt == 34
17 Jun 2022 13:36:24,488 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: k1. sink.event.drain.sucess == 34
17 Jun 2022 13:36:24,488 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: k1. sink.event.write.fail == 0
17 Jun 2022 13:36:24,488 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hive.HiveSink.stop:488)  - Hive Sink k1 stopped
17 Jun 2022 13:36:24,488 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel c1
17 Jun 2022 13:36:24,488 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: c1}
17 Jun 2022 13:36:24,488 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@77c174d2 counterGroup:{ name:null counters:{runner.interruptions=1, runner.backoffs.consecutive=318, runner.backoffs=666} } }
17 Jun 2022 13:36:24,488 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: c1 stopped
17 Jun 2022 13:36:24,489 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: c1. channel.start.time == 1655433898372
17 Jun 2022 13:36:24,489 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: c1. channel.stop.time == 1655444184488
17 Jun 2022 13:36:24,489 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: c1. channel.capacity == 1000
17 Jun 2022 13:36:24,489 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: c1. channel.current.size == 0
17 Jun 2022 13:36:24,489 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: c1. channel.event.put.attempt == 34
17 Jun 2022 13:36:24,489 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: c1. channel.event.put.success == 34
17 Jun 2022 13:36:24,490 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: c1. channel.event.take.attempt == 705
17 Jun 2022 13:36:24,490 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: c1. channel.event.take.success == 34
17 Jun 2022 13:36:24,490 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 17
17 Jun 2022 13:36:24,491 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
